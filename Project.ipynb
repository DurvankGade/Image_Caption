{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "30d34e8e-b849-42f7-aa66-c9a9facb7568",
   "metadata": {},
   "source": [
    "<h1 align = \"Center\">Image Captioning Using VGG16</h1>\n",
    "<h5><b>Authors:</b> <ul><li>Suryakiran R</li><br><li>Durvank Gade</li><br><li>Nikhil Bansal</li><br><li>Rishvan Rajavel</li><br></ul></h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "452abbf7-7455-43d8-b0a7-025f86461a76",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import os\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.applications.vgg16 import preprocess_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6b7917cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_file = \"Data/Images/flickr30k_images\"\n",
    "captions_file = \"Data/captions.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "deb06485",
   "metadata": {},
   "outputs": [],
   "source": [
    "cap_df = pd.read_csv(captions_file)\n",
    "cap_df.columns = ['image','caption']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "be425cbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_image(image_file, target_size=(224, 224)):\n",
    "    img = Image.open(image_file).resize(target_size)\n",
    "    img = np.array(img)\n",
    "    if img.shape[-1] == 4:  # If the image has an alpha channel, remove it\n",
    "        img = img[..., :3]\n",
    "    img = preprocess_input(img)  # VGG16 preprocessing\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4a706f3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_captions(captions, max_length=20):\n",
    "    tokenizer = Tokenizer(num_words=5000, oov_token='<OOV>')\n",
    "    tokenizer.fit_on_texts(captions)\n",
    "    sequences = tokenizer.texts_to_sequences(captions)\n",
    "    padded_sequences = pad_sequences(sequences, maxlen=max_length, padding='post')\n",
    "    return tokenizer, padded_sequences\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "09b2c6c7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "captions_list = cap_df['caption'].astype(str).tolist()\n",
    "tokenizer, padded_captions = preprocess_captions(captions_list)\n",
    "\n",
    "preprocessed_images = []\n",
    "for img_name in cap_df['image'][0:5000]:\n",
    "    img_path = image_file + '/' + img_name\n",
    "    preprocessed_images.append(preprocess_image(img_path))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "30c88fd6-e466-467f-895d-40476129b85e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for img_name in cap_df['image'][5000:10000]:\n",
    "    img_path = image_file + '/' + img_name\n",
    "    preprocessed_images.append(preprocess_image(img_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "38ada780-72c6-4fd7-8878-8d8366dbb703",
   "metadata": {},
   "outputs": [],
   "source": [
    "for img_name in cap_df['image'][10000:15000]:\n",
    "    img_path = image_file + '/' + img_name\n",
    "    preprocessed_images.append(preprocess_image(img_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "701ded25-b5fe-4e0e-adbd-d09c3a304126",
   "metadata": {},
   "outputs": [],
   "source": [
    "for img_name in cap_df['image'][15000:20000]:\n",
    "    img_path = image_file + '/' + img_name\n",
    "    preprocessed_images.append(preprocess_image(img_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7040ea3f-5fd5-48c6-b21b-db7bb0a6790e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for img_name in cap_df['image'][20000:25000]:\n",
    "    img_path = image_file + '/' + img_name\n",
    "    preprocessed_images.append(preprocess_image(img_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "90e99f9d-c0b2-4051-b9c8-0921b00603d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "for img_name in cap_df['image'][25000:30000]:\n",
    "    img_path = image_file + '/' + img_name\n",
    "    preprocessed_images.append(preprocess_image(img_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a1a9e246",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_images = np.array(preprocessed_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b275fa3b-63b9-4583-bbf8-43da01397828",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels.h5\n",
      "\u001b[1m553467096/553467096\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 0us/step\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.applications.vgg16 import VGG16, preprocess_input\n",
    "from tensorflow.keras.models import Model\n",
    "base_model = VGG16(weights='imagenet')\n",
    "model = Model(inputs=base_model.input, outputs=base_model.get_layer('fc2').output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2de71253",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1674s\u001b[0m 11s/step\n",
      "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m542s\u001b[0m 3s/step\n",
      "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m503s\u001b[0m 3s/step\n",
      "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1009s\u001b[0m 6s/step\n",
      "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1046s\u001b[0m 7s/step\n",
      "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m513s\u001b[0m 3s/step\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'pickle' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 23\u001b[0m\n\u001b[0;32m     21\u001b[0m np\u001b[38;5;241m.\u001b[39msave(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpadded_captions.npy\u001b[39m\u001b[38;5;124m'\u001b[39m, padded_captions)\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtokenizer.pkl\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwb\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m handle:\n\u001b[1;32m---> 23\u001b[0m     \u001b[43mpickle\u001b[49m\u001b[38;5;241m.\u001b[39mdump(tokenizer, handle, protocol\u001b[38;5;241m=\u001b[39mpickle\u001b[38;5;241m.\u001b[39mHIGHEST_PROTOCOL)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'pickle' is not defined"
     ]
    }
   ],
   "source": [
    "# def extract_features(preprocessed_images):\n",
    "#     features = model.predict(preprocessed_images, batch_size=32, verbose=1)\n",
    "#     return features\n",
    "\n",
    "# features = extract_features(preprocessed_images)\n",
    "def extract_features_in_chunks(preprocessed_images, chunk_size=5000):\n",
    "    num_images = preprocessed_images.shape[0]\n",
    "    features = []\n",
    "\n",
    "    for start_idx in range(0, num_images, chunk_size):\n",
    "        end_idx = min(start_idx + chunk_size, num_images)\n",
    "        batch_images = preprocessed_images[start_idx:end_idx]\n",
    "        batch_features = model.predict(batch_images, batch_size=32, verbose=1)\n",
    "        features.append(batch_features)\n",
    "    \n",
    "    return np.vstack(features)\n",
    "\n",
    "# Example usage:\n",
    "features = extract_features_in_chunks(preprocessed_images, chunk_size=5000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0ffe27ff-fc8e-453f-9343-8f8f105d98d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "29e75216-0fdc-4098-9a2d-bdf252b0b062",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('features.npy', features)\n",
    "np.save('padded_captions.npy', padded_captions)\n",
    "with open('tokenizer.pkl', 'wb') as handle:\n",
    "    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ca9cc14d-3659-40ee-a390-38c3a8fe8116",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features shape: (30000, 4096)\n",
      "Features type: <class 'numpy.ndarray'>\n",
      "[[2.7699418  2.890111   0.         ... 0.         0.01797792 1.4492029 ]\n",
      " [2.7699418  2.890111   0.         ... 0.         0.01797792 1.4492029 ]\n",
      " [2.7699418  2.890111   0.         ... 0.         0.01797792 1.4492029 ]\n",
      " [2.7699418  2.890111   0.         ... 0.         0.01797792 1.4492029 ]\n",
      " [2.7699418  2.890111   0.         ... 0.         0.01797792 1.4492029 ]]\n"
     ]
    }
   ],
   "source": [
    "features = np.load('features.npy')\n",
    "\n",
    "# Check the shape and type of features\n",
    "print(f\"Features shape: {features.shape}\")\n",
    "print(f\"Features type: {type(features)}\")\n",
    "\n",
    "# Print the first few entries to verify content\n",
    "print(features[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14753704-0970-415e-89ae-029649ff46e0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
